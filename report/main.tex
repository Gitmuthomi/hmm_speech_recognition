\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\title{Speech Recognition with Hidden Markov Models (HMMs)}
\author{Group Project Report}
\date{\today}

\begin{document}

\maketitle

\section*{Introduction}

This report presents our work on building a speech recognition system using \textbf{Hidden Markov Models (HMMs)}. Due to a widespread blackout in Spain during our academic term, we lost a full week of classes. Unfortunately, this affected our learning continuity, especially on advanced topics such as HMMs which were not fully covered before this assignment was given.

Despite these difficulties, we approached this project with the aim to learn practically. We relied on a Jupyter notebook shared publicly, which provided a technical walkthrough of HMM-based speech recognition. Our goal here is to explain what we understood and implemented in the most clear and human way possible.

\section*{What is a Hidden Markov Model?}

A \textbf{Hidden Markov Model} (HMM) is a tool used to model systems that change over time in a probabilistic way. It’s called “hidden” because we can’t directly observe the states of the system — we only see outcomes (called \textit{observations}) that are produced by those hidden states.

In speech recognition, the hidden states might represent different phonemes or sound units of speech, while the observations are measurements of the audio signal — for example, a type of feature called MFCC (Mel-Frequency Cepstral Coefficients).

An HMM is defined by:
\begin{itemize}
  \item A set of \textbf{states} (e.g., different sounds or phonemes).
  \item A \textbf{transition probability} matrix (the probability of moving from one state to another).
  \item An \textbf{emission probability} distribution (the probability of observing a feature given the hidden state).
  \item An \textbf{initial state distribution} (which state we start in).
\end{itemize}

The idea is to learn the parameters of this model from labeled audio data. Once trained, given a new sequence of audio features, we can use the model to determine which word (or phoneme sequence) was likely spoken.

\section*{Step-by-Step Pipeline}

The notebook we followed implements an end-to-end system for speech recognition using HMMs. Below, we summarize and expand each step we took.

\subsection*{1. Audio Input and Visualization}

We began by reading `.wav` files using the \texttt{scipy.io.wavfile} module. This gives us both:
\begin{itemize}
    \item The \textbf{sampling frequency} (e.g., 16,000 Hz).
    \item The raw \textbf{audio waveform}, which is just a sequence of numbers representing sound pressure.
\end{itemize}

We plotted the audio waveform to visualize how sound varies over time. This helps us understand the data before processing it.

\subsection*{2. Preprocessing and Normalization}

Speech signals can vary in loudness and clarity depending on how they were recorded. To make all samples more consistent, we:
\begin{itemize}
    \item Normalized the audio so its amplitude ranges between -1 and 1.
    \item Applied RMS (Root Mean Square) scaling to ensure uniform loudness.
\end{itemize}

We also handled stereo audio by selecting one channel or averaging both to create a mono signal.

\subsection*{3. Feature Extraction – MFCCs}

Rather than using the raw waveform (which has thousands of values), we extracted features called \textbf{MFCCs} (Mel-Frequency Cepstral Coefficients).

MFCCs are compact numerical representations of the speech signal that:
\begin{itemize}
    \item Emphasize perceptually important information.
    \item Reduce the dimensionality of the data.
    \item Are commonly used in speech and audio classification tasks.
\end{itemize}

We also computed \textbf{delta} and \textbf{delta-delta} features (which represent the speed and acceleration of the MFCCs over time), making our feature vectors more descriptive.

\subsection*{4. Data Augmentation}

To improve model robustness and reduce overfitting, we artificially created more audio data using:
\begin{itemize}
    \item \textbf{Pitch shifting} (making audio sound higher or lower).
    \item \textbf{Speed changes} (making speech faster or slower).
    \item \textbf{Time stretching} (changing duration without altering pitch).
\end{itemize}

This helped the model generalize better to different voices and recording conditions.

\subsection*{5. HMM Training per Word}

We trained a separate HMM for each word in the dataset. That means:
\begin{itemize}
    \item All audio examples for the word “yes” were used to train one HMM.
    \item All examples for “no” trained a different HMM.
\end{itemize}

Each HMM learned the statistical structure (patterns) of the audio features representing its corresponding word.

\subsection*{6. Classification via Scoring}

Once all HMMs were trained, we used them to classify new audio samples:
\begin{itemize}
    \item For each sample, we calculated the \textbf{log-likelihood score} under every HMM.
    \item The model that gave the highest score was assumed to be the predicted class.
\end{itemize}

To improve this further, these scores were used as features for a simple \textbf{logistic regression classifier}, providing a final decision.

\section*{Results and Evaluation}

We evaluated the system using:
\begin{itemize}
    \item \textbf{Confusion matrices} to see which words were confused.
    \item \textbf{Accuracy scores} to get a sense of overall performance.
\end{itemize}

Despite the complexity of speech recognition and our short timeline, the model showed encouraging results and worked reliably for basic commands.

\section*{Reflections and Conclusion}

This project was a valuable learning experience. Working under the pressure of a lost week due to a national power blackout — with reduced access to classes, guidance, and time — was extremely challenging.

We did not fully understand HMMs at the start. The theory is complex and we lacked time to deeply study the math behind it. However, by following a structured and practical approach, we were able to build a working system and, in the process, gained a meaningful understanding of how HMMs apply to real-world audio.

\subsection*{Things We Learned}

\begin{itemize}
    \item The importance of preprocessing and normalization in audio tasks.
    \item How MFCCs transform raw sound into meaningful features.
    \item The intuition of HMMs — modeling sequences of data with state transitions.
    \item That data augmentation is key for performance, even in simple models.
\end{itemize}

\subsection*{Final Thoughts}

This report does not present a perfect or deeply academic solution, but rather a sincere and resilient attempt by students facing real obstacles. We hope our work reflects a spirit of effort, learning, and adaptation.

\vspace{1em}

\noindent\textbf{Acknowledgment:} We thank the creator of the original notebook (Ashiq Nazir Bhat), whose clear and well-commented code was essential to our progress. We also thank our instructors for their flexibility during a difficult time.

\end{document}
